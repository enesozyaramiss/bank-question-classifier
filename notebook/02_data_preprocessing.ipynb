{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a148a7cf-b44b-4486-81e0-3f26297b4c5f",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Banking Customer Questions Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc0611-39db-47ac-adbd-279feb94323c",
   "metadata": {},
   "source": [
    "## 1. DATA LOADING & INITIAL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5db686-28f1-4005-b68f-f21c364f7833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. DATA LOADING & INITIAL SETUP\n",
      "------------------------------\n",
      "Loading raw data...\n",
      "   Raw data loaded: (1962, 8)\n",
      "   Questions: 1962\n",
      "   Unique departments: 15\n",
      "\n",
      "Current class distribution:\n",
      " 1. Пазар Ежедневно банкиране                 576 ( 29.4%)\n",
      " 2. Други                                     318 ( 16.2%)\n",
      " 3. Пазар Жилищни и ипотечни кредити          269 ( 13.7%)\n",
      " 4. Пазар Потребителско кредитиране           144 (  7.3%)\n",
      " 5. Пазар Разсрочени плащания                 120 (  6.1%)\n",
      " 6. Пазар Малък бизнес                         95 (  4.8%)\n",
      " 7. ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ                  83 (  4.2%)\n",
      " 8. Пазар Спестяване и инвестиции              83 (  4.2%)\n",
      " 9. Няколко пазара                             70 (  3.6%)\n",
      "10. Пазар Банково застраховане                 66 (  3.4%)\n",
      "11. ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОСТ        61 (  3.1%)\n",
      "12. ПРАВНО                                     34 (  1.7%)\n",
      "13. ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛНИ КЛИЕ   33 (  1.7%)\n",
      "14. CRM                                         9 (  0.5%)\n",
      "15. ДСК Лизинг                                  1 (  0.1%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"1. DATA LOADING & INITIAL SETUP\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the raw data\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_excel('../data/raw/Коментари за сортиране.xlsx', header=None, skiprows=1)\n",
    "df.columns = ['index', 'message_uid', 'created_dttm', 'question', 'department', 'col6', 'col7', 'market_segment']\n",
    "\n",
    "print(f\"   Raw data loaded: {df.shape}\")\n",
    "print(f\"   Questions: {len(df)}\")\n",
    "print(f\"   Unique departments: {df['department'].nunique()}\")\n",
    "\n",
    "# Display current class distribution\n",
    "print(f\"\\nCurrent class distribution:\")\n",
    "class_dist = df['department'].value_counts()\n",
    "for i, (dept, count) in enumerate(class_dist.items(), 1):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {dept[:40]:<40} {count:4d} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac324d-597f-4d92-b18e-be139aeffe6f",
   "metadata": {},
   "source": [
    "## 2. CLASS CONSOLIDATION STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e541a4c-7408-4299-b26e-7813c477f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Analysis:\n",
      "   ДСК Лизинг: 1 sample (impossible to train/test)\n",
      "   CRM: 9 samples (high overfitting risk)\n",
      "   ПРАВНО: 34 samples (borderline for robust training)\n",
      "   Extreme imbalance ratio: 576:1\n",
      "\n",
      "Solution: Consolidate Support Services\n",
      "   Combining low-volume support departments into 'Support_Services'\n",
      "\n",
      "After consolidation:\n",
      " 1. Пазар Ежедневно банкиране                 576 ( 29.4%)\n",
      " 2. Други                                     318 ( 16.2%)\n",
      " 3. Пазар Жилищни и ипотечни кредити          269 ( 13.7%)\n",
      " 4. Пазар Потребителско кредитиране           144 (  7.3%)\n",
      " 5. Пазар Разсрочени плащания                 120 (  6.1%)\n",
      " 6. Пазар Малък бизнес                         95 (  4.8%)\n",
      " 7. ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ                  83 (  4.2%)\n",
      " 8. Пазар Спестяване и инвестиции              83 (  4.2%)\n",
      " 9. Няколко пазара                             70 (  3.6%)\n",
      "10. Пазар Банково застраховане                 66 (  3.4%)\n",
      "11. ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОСТ        61 (  3.1%)\n",
      "12. Support_Services                           44 (  2.2%)\n",
      "13. ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛНИ КЛИЕ   33 (  1.7%)\n",
      "\n",
      "Class consolidation summary:\n",
      "   Classes reduced: 15 → 13\n",
      "   Minimum class size: 33 samples\n",
      "   New imbalance ratio: 576:33 (was 576:1)\n",
      "   Support_Services created from: ПРАВНО + CRM + ДСК Лизинг = 44 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Problem Analysis:\")\n",
    "print(\"   ДСК Лизинг: 1 sample (impossible to train/test)\")\n",
    "print(\"   CRM: 9 samples (high overfitting risk)\")\n",
    "print(\"   ПРАВНО: 34 samples (borderline for robust training)\")\n",
    "print(\"   Extreme imbalance ratio: 576:1\")\n",
    "\n",
    "print(f\"\\nSolution: Consolidate Support Services\")\n",
    "print(\"   Combining low-volume support departments into 'Support_Services'\")\n",
    "\n",
    "# Define class mapping strategy\n",
    "CLASS_MAPPING = {\n",
    "    # Support services consolidation (business logic: non-customer-facing support)\n",
    "    'ПРАВНО': 'Support_Services',           # Legal support: 34 samples\n",
    "    'CRM': 'Support_Services',              # Customer relationship: 9 samples  \n",
    "    'ДСК Лизинг': 'Support_Services',       # Leasing services: 1 sample\n",
    "    \n",
    "    # Keep all other departments as-is (sufficient sample sizes)\n",
    "    'Пазар Ежедневно банкиране': 'Пазар Ежедневно банкиране',\n",
    "    'Други': 'Други',\n",
    "    'Пазар Жилищни и ипотечни кредити': 'Пазар Жилищни и ипотечни кредити',\n",
    "    'Пазар Потребителско кредитиране': 'Пазар Потребителско кредитиране',\n",
    "    'Пазар Разсрочени плащания': 'Пазар Разсрочени плащания',\n",
    "    'Пазар Малък бизнес': 'Пазар Малък бизнес',\n",
    "    'ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ': 'ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ',\n",
    "    'Пазар Спестяване и инвестиции': 'Пазар Спестяване и инвестиции',\n",
    "    'Няколко пазара': 'Няколко пазара',\n",
    "    'Пазар Банково застраховане': 'Пазар Банково застраховане',\n",
    "    'ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОСТ': 'ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОСТ',\n",
    "    'ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛНИ КЛИЕНТИ': 'ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛНИ КЛИЕНТИ'\n",
    "}\n",
    "\n",
    "# Apply class mapping\n",
    "df['department_consolidated'] = df['department'].map(CLASS_MAPPING)\n",
    "\n",
    "# Verify consolidation\n",
    "print(f\"\\nAfter consolidation:\")\n",
    "consolidated_dist = df['department_consolidated'].value_counts()\n",
    "for i, (dept, count) in enumerate(consolidated_dist.items(), 1):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{i:2d}. {dept[:40]:<40} {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass consolidation summary:\")\n",
    "print(f\"   Classes reduced: {len(class_dist)} → {len(consolidated_dist)}\")\n",
    "print(f\"   Minimum class size: {consolidated_dist.min()} samples\")\n",
    "print(f\"   New imbalance ratio: {consolidated_dist.max()}:{consolidated_dist.min()} (was 576:1)\")\n",
    "print(f\"   Support_Services created from: ПРАВНО + CRM + ДСК Лизинг = 44 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a7ed6-665a-4e61-81e3-da069d379f08",
   "metadata": {},
   "source": [
    "## 3. TEXT QUALITY ANALYSIS & CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef91990f-04b2-4d8c-bfde-116a2ff4ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Very short questions (<10 chars): 29\n",
      "     Examples:\n",
      "       'в bankway' → Други\n",
      "       'да' → Други\n",
      "       'трезор' → Пазар Ежедневно банкиране\n",
      "       'ДОбър ден' → Други\n",
      "       'Драстиии' → Други\n",
      "   Very long questions (>500 chars): 17\n",
      "   Null/missing questions: 0\n",
      "   Questions with non-Bulgarian chars: 297\n"
     ]
    }
   ],
   "source": [
    "# Very short questions\n",
    "short_questions = df[df['question'].str.len() < 10]\n",
    "print(f\"   Very short questions (<10 chars): {len(short_questions)}\")\n",
    "if len(short_questions) > 0:\n",
    "    print(\"     Examples:\")\n",
    "    for i, row in short_questions.head(5).iterrows():\n",
    "        print(f\"       '{row['question']}' → {row['department_consolidated']}\")\n",
    "\n",
    "# Very long questions (potential outliers)\n",
    "long_questions = df[df['question'].str.len() > 500]\n",
    "print(f\"   Very long questions (>500 chars): {len(long_questions)}\")\n",
    "\n",
    "# Missing or null questions\n",
    "null_questions = df['question'].isnull().sum()\n",
    "print(f\"   Null/missing questions: {null_questions}\")\n",
    "\n",
    "# Character encoding issues (non-Bulgarian characters)\n",
    "bulgarian_pattern = re.compile(r'^[а-яА-ЯёЁ\\s\\d.,!?;:()\"\\'-]+$')\n",
    "non_bulgarian = df[~df['question'].str.match(bulgarian_pattern, na=False)]\n",
    "print(f\"   Questions with non-Bulgarian chars: {len(non_bulgarian)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661186-9938-449f-8b46-5d912680a771",
   "metadata": {},
   "source": [
    "## 4.TEXT CLEANING & FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc4957b-b0e0-4430-9bf2-470ec959f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text cleaning strategy:\n",
      "   Applying Bulgarian text normalization...\n",
      "   Handling very short questions...\n",
      "     Flagged 29 short questions for review\n",
      "\n",
      "Text statistics after cleaning:\n",
      "   Average length: 65.7 characters\n",
      "   Median length: 50.0 characters\n",
      "   Min length: 2 characters\n",
      "   Max length: 4094 characters\n",
      "   Questions <10 chars: 29\n",
      "Final quality assessment:\n",
      "   Duplicate questions (after cleaning): 10\n",
      "       Raw: 'Когато има технически проблем с АТМ, какви такси с...'\n",
      "       Cleaned: 'когато има технически проблем с атм, какви такси с...'\n",
      "       Department: ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ\n",
      "       ---\n",
      "       Raw: 'Когато има технически проблем с АТМ, какви такси с...'\n",
      "       Cleaned: 'когато има технически проблем с атм, какви такси с...'\n",
      "       Department: Пазар Ежедневно банкиране\n",
      "       ---\n",
      "       Raw: 'колко дебитни карти може да има един клиент...'\n",
      "       Cleaned: 'колко дебитни карти може да има един клиент...'\n",
      "       Department: Пазар Ежедневно банкиране\n",
      "       ---\n",
      "       Raw: 'колко дебитни карти може да има един клиент ...'\n",
      "       Cleaned: 'колко дебитни карти може да има един клиент...'\n",
      "       Department: Пазар Ежедневно банкиране\n",
      "       ---\n",
      "     Action: Removing duplicates, keeping first occurrence\n",
      "     Dataset size after deduplication: 1957 questions\n",
      "   Empty questions (after cleaning): 0\n",
      "   Minimum class size: 33 samples\n",
      "   All classes trainable: Yes\n",
      "\n",
      "Data quality summary:\n",
      "   Clean questions ready: 1957\n",
      "   Consolidated classes: 13\n",
      "   Ready for train/test split: Yes\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nText cleaning strategy:\")\n",
    "\n",
    "# Define text cleaning function\n",
    "def clean_bulgarian_text(text):\n",
    "    \"\"\"Clean Bulgarian text for NLP processing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove excessive punctuation (keep single punctuation)\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    # Normalize quotes\n",
    "    text = re.sub(r'[\"\"„\"]', '\"', text)\n",
    "    text = re.sub(r'[''`]', \"'\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "print(\"   Applying Bulgarian text normalization...\")\n",
    "df['question_cleaned'] = df['question'].apply(clean_bulgarian_text)\n",
    "\n",
    "# Handle very short questions\n",
    "print(\"   Handling very short questions...\")\n",
    "df['is_short_question'] = df['question_cleaned'].str.len() < 10\n",
    "print(f\"     Flagged {df['is_short_question'].sum()} short questions for review\")\n",
    "\n",
    "# Quality check after cleaning\n",
    "cleaned_lengths = df['question_cleaned'].str.len()\n",
    "print(f\"\\nText statistics after cleaning:\")\n",
    "print(f\"   Average length: {cleaned_lengths.mean():.1f} characters\")\n",
    "print(f\"   Median length: {cleaned_lengths.median():.1f} characters\")\n",
    "print(f\"   Min length: {cleaned_lengths.min()} characters\")\n",
    "print(f\"   Max length: {cleaned_lengths.max()} characters\")\n",
    "print(f\"   Questions <10 chars: {(cleaned_lengths < 10).sum()}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(\"Final quality assessment:\")\n",
    "\n",
    "# Duplicate questions after cleaning\n",
    "duplicate_cleaned = df.duplicated(subset=['question_cleaned'], keep=False).sum()\n",
    "print(f\"   Duplicate questions (after cleaning): {duplicate_cleaned}\")\n",
    "\n",
    "if duplicate_cleaned > 0:\n",
    "    duplicates_df = df[df.duplicated(subset=['question_cleaned'], keep=False)].sort_values('question_cleaned')\n",
    "    if len(duplicates_df) > 0:\n",
    "        for i, (idx, row) in enumerate(duplicates_df.head(4).iterrows()):\n",
    "            print(f\"       Raw: '{row['question'][:50]}...'\")\n",
    "            print(f\"       Cleaned: '{row['question_cleaned'][:50]}...'\")\n",
    "            print(f\"       Department: {row['department_consolidated']}\")\n",
    "            if i < len(duplicates_df) - 1:\n",
    "                print(f\"       ---\")\n",
    "    \n",
    "    # Decision: Remove duplicates for cleaner training\n",
    "    print(f\"     Action: Removing duplicates, keeping first occurrence\")\n",
    "    df = df.drop_duplicates(subset=['question_cleaned'], keep='first')\n",
    "    print(f\"     Dataset size after deduplication: {len(df)} questions\")\n",
    "\n",
    "# Empty questions after cleaning\n",
    "empty_cleaned = (df['question_cleaned'].str.len() == 0).sum()\n",
    "print(f\"   Empty questions (after cleaning): {empty_cleaned}\")\n",
    "\n",
    "# Class distribution validation\n",
    "min_class_size = consolidated_dist.min()\n",
    "print(f\"   Minimum class size: {min_class_size} samples\")\n",
    "print(f\"   All classes trainable: {'Yes' if min_class_size >= 5 else 'No'}\")\n",
    "\n",
    "print(f\"\\nData quality summary:\")\n",
    "print(f\"   Clean questions ready: {len(df) - empty_cleaned}\")\n",
    "print(f\"   Consolidated classes: {len(consolidated_dist)}\")\n",
    "print(f\"   Ready for train/test split: {'Yes' if min_class_size >= 10 else 'Risky'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd96cc-98f9-4275-94c6-eabe6aa6bf03",
   "metadata": {},
   "source": [
    "## 5. TRAIN/TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3bdc17-fab6-44c7-a901-4ead7b34a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for train/test split...\n",
      "   Removed 0 empty questions\n",
      "   Final dataset size: 1957 questions\n",
      "\n",
      "Modeling dataset prepared:\n",
      "   Primary feature: question_cleaned (text)\n",
      "   Target: department_consolidated\n",
      "   Additional metadata will be preserved for analysis\n",
      "\n",
      "Class distribution for splitting:\n",
      "   • Пазар Ежедневно банкиране            572 ( 29.2%)\n",
      "   • Други                                317 ( 16.2%)\n",
      "   • Пазар Жилищни и ипотечни кредити     269 ( 13.7%)\n",
      "   • Пазар Потребителско кредитиране      144 (  7.4%)\n",
      "   • Пазар Разсрочени плащания            120 (  6.1%)\n",
      "   • Пазар Малък бизнес                    95 (  4.9%)\n",
      "   • ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ             83 (  4.2%)\n",
      "   • Пазар Спестяване и инвестиции         83 (  4.2%)\n",
      "   • Няколко пазара                        70 (  3.6%)\n",
      "   • Пазар Банково застраховане            66 (  3.4%)\n",
      "   • ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОСТ   61 (  3.1%)\n",
      "   • Support_Services                      44 (  2.2%)\n",
      "   • ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛНИ   33 (  1.7%)\n",
      "\n",
      "Split feasibility analysis:\n",
      "   Test size: 0.2 (20.0%)\n",
      "   Minimum samples per class: 33\n",
      "   Minimum test samples per class: 6\n",
      "   Stratified split possible: Yes\n",
      "\n",
      "Stratified split successful!\n",
      "   • Training set: 1565 questions\n",
      "   • Test set: 392 questions\n",
      "\n",
      "Train/test distribution verification:\n",
      "Class                               Train    Test     Train%   Test%\n",
      "----------------------------------------------------------------------\n",
      "Support_Services                    35       9        2.2      2.3     \n",
      "ДИРЕКЦИЯ БАНКОВИ ОПЕРАЦИИ           66       17       4.2      4.3     \n",
      "ДИРЕКЦИЯ КРЕДИТЕН РИСК ИНДИВИДУАЛН  26       7        1.7      1.8     \n",
      "ДИРЕКЦИЯ ПАЗАР АТМ И КАСОВА ДЕЙНОС  49       12       3.1      3.1     \n",
      "Други                               254      63       16.2     16.1    \n",
      "Няколко пазара                      56       14       3.6      3.6     \n",
      "Пазар Банково застраховане          53       13       3.4      3.3     \n",
      "Пазар Ежедневно банкиране           458      114      29.3     29.1    \n",
      "Пазар Жилищни и ипотечни кредити    215      54       13.7     13.8    \n",
      "Пазар Малък бизнес                  76       19       4.9      4.8     \n",
      "Пазар Потребителско кредитиране     115      29       7.3      7.4     \n",
      "Пазар Разсрочени плащания           96       24       6.1      6.1     \n",
      "Пазар Спестяване и инвестиции       66       17       4.2      4.3     \n"
     ]
    }
   ],
   "source": [
    "# Prepare data for splitting\n",
    "print(\"Preparing for train/test split...\")\n",
    "\n",
    "# Remove empty questions if any\n",
    "clean_df = df[df['question_cleaned'].str.len() > 0].copy()\n",
    "print(f\"   Removed {len(df) - len(clean_df)} empty questions\")\n",
    "print(f\"   Final dataset size: {len(clean_df)} questions\")\n",
    "\n",
    "# Features and target for modeling\n",
    "X = clean_df['question_cleaned']        # Primary feature for classification\n",
    "y = clean_df['department_consolidated'] # Target variable\n",
    "\n",
    "print(f\"\\nModeling dataset prepared:\")\n",
    "print(f\"   Primary feature: question_cleaned (text)\")\n",
    "print(f\"   Target: department_consolidated\")\n",
    "print(f\"   Additional metadata will be preserved for analysis\")\n",
    "\n",
    "print(f\"\\nClass distribution for splitting:\")\n",
    "final_class_dist = y.value_counts()\n",
    "for dept, count in final_class_dist.items():\n",
    "    percentage = (count / len(clean_df)) * 100\n",
    "    print(f\"   • {dept[:35]:<35} {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Check if stratified split is possible\n",
    "min_samples_per_class = final_class_dist.min()\n",
    "test_size = 0.2\n",
    "min_test_samples = int(min_samples_per_class * test_size)\n",
    "\n",
    "print(f\"\\nSplit feasibility analysis:\")\n",
    "print(f\"   Test size: {test_size} ({test_size*100}%)\")\n",
    "print(f\"   Minimum samples per class: {min_samples_per_class}\")\n",
    "print(f\"   Minimum test samples per class: {min_test_samples}\")\n",
    "print(f\"   Stratified split possible: {'Yes' if min_test_samples >= 1 else 'No'}\")\n",
    "\n",
    "# Perform stratified split\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStratified split successful!\")\n",
    "    print(f\"   • Training set: {len(X_train)} questions\")\n",
    "    print(f\"   • Test set: {len(X_test)} questions\")\n",
    "    \n",
    "    # Verify stratification\n",
    "    print(f\"\\nTrain/test distribution verification:\")\n",
    "    train_dist = y_train.value_counts().sort_index()\n",
    "    test_dist = y_test.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"{'Class':<35} {'Train':<8} {'Test':<8} {'Train%':<8} {'Test%'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for class_name in train_dist.index:\n",
    "        train_count = train_dist[class_name]\n",
    "        test_count = test_dist[class_name] if class_name in test_dist.index else 0\n",
    "        train_pct = (train_count / len(X_train)) * 100\n",
    "        test_pct = (test_count / len(X_test)) * 100 if test_count > 0 else 0\n",
    "        print(f\"{class_name[:34]:<35} {train_count:<8} {test_count:<8} {train_pct:<8.1f} {test_pct:<8.1f}\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"Stratified split failed: {e}\")\n",
    "    print(\"   Falling back to random split...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"   • Training set: {len(X_train)} questions\")\n",
    "    print(f\"   • Test set: {len(X_test)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abaddd7-7b85-4832-abdb-5f489b993492",
   "metadata": {},
   "source": [
    "## 6. DATA EXPORT & SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b40ccd1-fa41-4aff-9858-169fa9f0b410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features for model development...\n",
      "Basic text features created:\n",
      "   Text length statistics: mean=65.8, std=123.2\n",
      "   Word count statistics: mean=11.2, std=19.8\n",
      "   Questions with '?': 403 (20.6%)\n",
      "   Questions with numbers: 193 (9.9%)\n",
      "Preparing datasets for export...\n",
      "\n",
      "Dataset structure:\n",
      "   Modeling columns: question_cleaned, department_target\n",
      "   Analysis columns: message_uid, question_original, department_original, is_short_question\n",
      "   Use modeling columns for training/testing\n",
      "   Use analysis columns for error analysis and business insights\n",
      "Data exported successfully:\n",
      "   Train dataset: ../data/processed/train_data.csv (1565 rows)\n",
      "   Test dataset: ../data/processed/test_data.csv (392 rows)\n",
      "   Class mapping: ../data/processed/class_mapping.csv\n",
      "   Format: Modeling columns + Analysis metadata\n",
      "   Ready for both training and comprehensive evaluation\n",
      "\n",
      "PREPROCESSING SUMMARY:\n",
      "   Original dataset: 1957 questions, 15 classes\n",
      "   Final dataset: 1957 questions, 13 classes\n",
      "   Class consolidation: Support_Services = ПРАВНО + CRM + ДСК Лизинг\n",
      "   Text cleaning: Bulgarian normalization applied\n",
      "   Train/test split: 1565/392 (stratified)\n",
      "   Minimum class size: 33 samples\n",
      "   Imbalance ratio: 576:33\n",
      "\n",
      "READY FOR MODEL DEVELOPMENT:\n",
      "   Text classification: 13-class problem\n",
      "   Features: Clean Bulgarian text + basic text features\n",
      "   Target: Consolidated departments\n",
      "   Data quality: High (no duplicates, clean text)\n",
      "   Next step: 03_model_development.ipynb\n",
      "\n",
      "Preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing features for model development...\")\n",
    "\n",
    "# Create feature engineering dataset\n",
    "feature_df = clean_df.copy()\n",
    "\n",
    "# Basic text features\n",
    "feature_df['text_length'] = feature_df['question_cleaned'].str.len()\n",
    "feature_df['word_count'] = feature_df['question_cleaned'].str.split().str.len()\n",
    "feature_df['avg_word_length'] = feature_df['text_length'] / feature_df['word_count']\n",
    "\n",
    "# Question type indicators\n",
    "feature_df['has_question_mark'] = feature_df['question_cleaned'].str.contains(r'\\?', na=False)  \n",
    "feature_df['has_numbers'] = feature_df['question_cleaned'].str.contains(r'\\d', na=False)\n",
    "feature_df['has_uppercase'] = feature_df['question'].str.contains(r'[А-Я]', na=False)\n",
    "\n",
    "print(f\"Basic text features created:\")\n",
    "print(f\"   Text length statistics: mean={feature_df['text_length'].mean():.1f}, std={feature_df['text_length'].std():.1f}\")\n",
    "print(f\"   Word count statistics: mean={feature_df['word_count'].mean():.1f}, std={feature_df['word_count'].std():.1f}\")\n",
    "print(f\"   Questions with '?': {feature_df['has_question_mark'].sum()} ({(feature_df['has_question_mark'].mean()*100):.1f}%)\")\n",
    "print(f\"   Questions with numbers: {feature_df['has_numbers'].sum()} ({(feature_df['has_numbers'].mean()*100):.1f}%)\")\n",
    "\n",
    "# Prepare final datasets for export\n",
    "print(\"Preparing datasets for export...\")\n",
    "\n",
    "# Training dataset - Modeling + Analysis\n",
    "train_df = pd.DataFrame({\n",
    "    # Core modeling columns\n",
    "    'question_cleaned': X_train,\n",
    "    'department_target': y_train,\n",
    "    \n",
    "    # Analysis and debugging columns\n",
    "    'message_uid': clean_df.loc[X_train.index, 'message_uid'],\n",
    "    'question_original': clean_df.loc[X_train.index, 'question'],\n",
    "    'department_original': clean_df.loc[X_train.index, 'department'],\n",
    "    'is_short_question': clean_df.loc[X_train.index, 'is_short_question']\n",
    "})\n",
    "\n",
    "# Test dataset - Modeling + Analysis  \n",
    "test_df = pd.DataFrame({\n",
    "    # Core modeling columns\n",
    "    'question_cleaned': X_test,\n",
    "    'department_target': y_test,\n",
    "    \n",
    "    # Analysis and debugging columns\n",
    "    'message_uid': clean_df.loc[X_test.index, 'message_uid'],\n",
    "    'question_original': clean_df.loc[X_test.index, 'question'],\n",
    "    'department_original': clean_df.loc[X_test.index, 'department'],\n",
    "    'is_short_question': clean_df.loc[X_test.index, 'is_short_question']\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"   Modeling columns: question_cleaned, department_target\")\n",
    "print(f\"   Analysis columns: message_uid, question_original, department_original, is_short_question\")\n",
    "print(f\"   Use modeling columns for training/testing\")\n",
    "print(f\"   Use analysis columns for error analysis and business insights\")\n",
    "\n",
    "# Export to processed data folder\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "train_df.to_csv('../data/processed/train_data.csv', index=False, encoding='utf-8')\n",
    "test_df.to_csv('../data/processed/test_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Also save class mapping for future reference\n",
    "class_mapping_df = pd.DataFrame(list(CLASS_MAPPING.items()), columns=['original_department', 'consolidated_department'])\n",
    "class_mapping_df.to_csv('../data/processed/class_mapping.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data exported successfully:\")\n",
    "print(f\"   Train dataset: ../data/processed/train_data.csv ({len(train_df)} rows)\")\n",
    "print(f\"   Test dataset: ../data/processed/test_data.csv ({len(test_df)} rows)\")\n",
    "print(f\"   Class mapping: ../data/processed/class_mapping.csv\")\n",
    "print(f\"   Format: Modeling columns + Analysis metadata\")\n",
    "print(f\"   Ready for both training and comprehensive evaluation\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nPREPROCESSING SUMMARY:\")\n",
    "print(f\"   Original dataset: {len(df)} questions, {df['department'].nunique()} classes\")\n",
    "print(f\"   Final dataset: {len(clean_df)} questions, {len(consolidated_dist)} classes\")\n",
    "print(f\"   Class consolidation: Support_Services = ПРАВНО + CRM + ДСК Лизинг\")\n",
    "print(f\"   Text cleaning: Bulgarian normalization applied\")\n",
    "print(f\"   Train/test split: {len(X_train)}/{len(X_test)} (stratified)\")\n",
    "print(f\"   Minimum class size: {consolidated_dist.min()} samples\")\n",
    "print(f\"   Imbalance ratio: {consolidated_dist.max()}:{consolidated_dist.min()}\")\n",
    "\n",
    "print(f\"\\nREADY FOR MODEL DEVELOPMENT:\")\n",
    "print(f\"   Text classification: 13-class problem\")\n",
    "print(f\"   Features: Clean Bulgarian text + basic text features\")\n",
    "print(f\"   Target: Consolidated departments\")\n",
    "print(f\"   Data quality: High (no duplicates, clean text)\")\n",
    "print(f\"   Next step: 03_model_development.ipynb\")\n",
    "\n",
    "print(f\"\\nPreprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826af890-b6bf-44fb-9f9d-31fea35b7801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82a79b-87df-4055-b9b6-573a8fa4c676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a587-cc48-423e-bf15-7e00a3ec2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5edcf-7bff-4918-96c1-16c9bed7a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448bc910-2ffa-4832-a158-2da669ba6edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
